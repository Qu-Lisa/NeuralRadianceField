{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import imageio\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from nerf_model import TinyNerfModel\n",
    "from nerf_dataset import TinyCybertruckDataset\n",
    "from nerf_functions import get_rays, render_rays, get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA GeForce RTX 4080 SUPER\n"
     ]
    }
   ],
   "source": [
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyNerfModel(\n",
       "  (block1): Sequential(\n",
       "    (0): Linear(in_features=39, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (block2): Sequential(\n",
       "    (0): Linear(in_features=295, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=256, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp = \"\" # Add timestamp here. Will use the latest trial if empty\n",
    "output_dir = \"output\"\n",
    "if timestamp == \"\":\n",
    "    timestamp = sorted(os.listdir(output_dir))[-1] # Fetching the latest trial\n",
    "\n",
    "output_dir = os.path.join(output_dir, timestamp)\n",
    "model_file_name = sorted(os.listdir(output_dir))[-1] # Fetching the latest checkpoint\n",
    "checkpoint_state_dict = torch.load(os.path.join(output_dir, model_file_name))\n",
    "\n",
    "# load latest model\n",
    "model = TinyNerfModel().to(device) # Make sure to use same embedding size as the model used for training\n",
    "model.load_state_dict(checkpoint_state_dict)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pose_spherical(theta, phi, radius):\n",
    "    \"\"\"\n",
    "    Convert spherical coordinates to camera pose\n",
    "    Inputs:\n",
    "        theta: float, azimuthal angle in degrees\n",
    "        phi: float, polar angle in degrees\n",
    "        radius: float, distance from the origin\n",
    "    Outputs:\n",
    "        c2w: torch.tensor, 4x4 camera-to-world matrix\n",
    "    \"\"\"\n",
    "    trans_t = lambda t : torch.tensor([\n",
    "        [1,0,0,0],\n",
    "        [0,1,0,0],\n",
    "        [0,0,1,t],\n",
    "        [0,0,0,1],\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    rot_phi = lambda phi : torch.tensor([\n",
    "        [1,0,0,0],\n",
    "        [0,np.cos(phi),-np.sin(phi),0],\n",
    "        [0,np.sin(phi), np.cos(phi),0],\n",
    "        [0,0,0,1],\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    rot_theta = lambda th : torch.tensor([\n",
    "        [np.cos(th),0,-np.sin(th),0],\n",
    "        [0,1,0,0],\n",
    "        [np.sin(th),0, np.cos(th),0],\n",
    "        [0,0,0,1],\n",
    "    ], dtype=torch.float32)\n",
    "\n",
    "    c2w = trans_t(radius)\n",
    "    c2w = rot_phi(phi / 180. * np.pi) @ c2w\n",
    "    c2w = rot_theta(theta / 180. * np.pi) @ c2w\n",
    "    c2w = torch.tensor([[-1,0,0,0],[0,0,1,0],[0,1,0,0],[0,0,0,1]], dtype=torch.float32) @ c2w\n",
    "    return c2w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "testimg, _, testfocal = TinyCybertruckDataset(\"test\")[0]\n",
    "H, W = testimg.shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120/120 [00:11<00:00, 10.04it/s]\n",
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (100, 100) to (112, 112) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n",
      "[swscaler @ 0x62c7b40] Warning: data is not aligned! This can lead to a speed loss\n"
     ]
    }
   ],
   "source": [
    "frames = []\n",
    "\n",
    "for th in tqdm(np.linspace(0., 360., 120, endpoint=False)):\n",
    "    c2w = pose_spherical(theta=th, phi=-15., radius=5.).to(device)\n",
    "\n",
    "    rays_o, rays_d = get_rays(H, W, testfocal, c2w, device=device)\n",
    "\n",
    "    rgb = render_rays(model, rays_o, rays_d, near=3., far=7., N_samples=64, device=device)\n",
    "\n",
    "    img = rgb.detach().cpu().numpy()\n",
    "    img = np.clip(img, 0, 1)\n",
    "    frames.append((img*255).astype(np.uint8))\n",
    "\n",
    "imageio.mimwrite('tiny_nerf.mp4', frames, fps=30, quality=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nerf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
